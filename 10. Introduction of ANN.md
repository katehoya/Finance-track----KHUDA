## 10.1.3 Perceptron  
완전 연결 층 (fully connected layer) : input ~ threshold까지의 한 층.  
![image](https://github.com/user-attachments/assets/57c79388-2add-4fa9-988b-24d85c05606c)  
![image](https://github.com/user-attachments/assets/69a820af-815c-41d4-81e3-4c5d6800ee89)  
그러나 퍼셉트론은 단순한 논리합(XOR)과 같은 문제들을 풀 수 없다고 알려졌다.  
이를 해결한 게 퍼셉트론을 여러 층으로 쌓아올린 MLP이다.  

## 10. 1.4 MLP  
![image](https://github.com/user-attachments/assets/f0278149-8f87-45c1-b195-cb2f4a398a60)  
DNN(Deep NN) : hidden layer를 여러 개 쌓아올린 인공 신경망.  
역전파 알고리즘 : 
1. 먼저 미니배치에 대한 정방향 계산을 수행.
2. 오차를 측정.
3. 역방향으로 거치며 각 연결이 오차에 기여한 정도를 측정.
4. 마지막으로 이 오차가 감소하도록 가중치와 편향을 조정.(gradient descent) --> 기존의 계단 함수에서 좀 더 soft한 sigmoid, tanh, ReLU함수로 threshold 변경.
![image](https://github.com/user-attachments/assets/2e6a3ba2-faa5-43e0-bdec-8b9bbeeae91a)
![image](https://github.com/user-attachments/assets/bf352cca-9980-400b-b03b-8fe704711f27)
![image](https://github.com/user-attachments/assets/5476412c-a2ec-4f10-a51e-fc4fec0c746a)
![image](https://github.com/user-attachments/assets/ac7a876f-74f1-4b6e-9e99-c9d5cf2ff429)

## 10.2 MLP 구현  
![image](https://github.com/user-attachments/assets/e4767547-262f-4513-9da4-4ce8888923ab)


## 10.3 NeuralNet 하이퍼파라미터 튜닝하기  



