## 10.1.3 Perceptron  
완전 연결 층 (fully connected layer) : input ~ threshold까지의 한 층.  
![image](https://github.com/user-attachments/assets/57c79388-2add-4fa9-988b-24d85c05606c)  
![image](https://github.com/user-attachments/assets/69a820af-815c-41d4-81e3-4c5d6800ee89)  
그러나 퍼셉트론은 단순한 논리합(XOR)과 같은 문제들을 풀 수 없다고 알려졌다.  
이를 해결한 게 퍼셉트론을 여러 층으로 쌓아올린 MLP이다.  

## 10. 1.4 MLP  
![image](https://github.com/user-attachments/assets/f0278149-8f87-45c1-b195-cb2f4a398a60)  
DNN(Deep NN) : hidden layer를 여러 개 쌓아올린 인공 신경망.  
역전파 알고리즘 : 
1. 먼저 미니배치에 대한 정방향 계산을 수행.
2. 오차를 측정.
3. 역방향으로 거치며 각 연결이 오차에 기여한 정도를 측정.
4. 마지막으로 이 오차가 감소하도록 가중치와 편향을 조정.(gradient descent) --> 기존의 계단 함수에서 좀 더 soft한 sigmoid, tanh, ReLU함수로 threshold 변경.
![image](https://github.com/user-attachments/assets/2e6a3ba2-faa5-43e0-bdec-8b9bbeeae91a)
![image](https://github.com/user-attachments/assets/bf352cca-9980-400b-b03b-8fe704711f27)
![image](https://github.com/user-attachments/assets/5476412c-a2ec-4f10-a51e-fc4fec0c746a)
![image](https://github.com/user-attachments/assets/ac7a876f-74f1-4b6e-9e99-c9d5cf2ff429)

## 10.2 MLP 구현  
![image](https://github.com/user-attachments/assets/e4767547-262f-4513-9da4-4ce8888923ab)


## 10.3 NeuralNet 하이퍼파라미터 튜닝하기  
***  
# 11. DNN training   
## 11.1 그래디언트 소실과 폭주 문제  
그래디언트 소실 : GD에서 하위 층의 연결 가중치를 변경하지 않고 그대로 유지.  
그래디언트 폭주 : 그래디언트가 점점 커져서 비정상적으로 큰 가중치로 update되는 경우.  
주요 원인 : sigmoid 활성화 함수와 평균이 0이고 표준편차가 1인 정규 분포 의 조합.   
sigmoid 함수가 평균이0이 아니라 0.5이고, 0 or 1에 갈수록 기울기가 0에 가까워지는 이유 때문.  

### 11.1.1 글로럿과 He 초기화  
불안정한 그래디언트를 안정화하려면 각 층의 출력에 대한 분산 = 입력에 대한 분산 이어야 한다.  
그리고 역방향에서 층을 통과하기 전과 후의 그래디언트 분산이 동일해야 한다.  
![image](https://github.com/user-attachments/assets/249a1a9d-b9c8-44eb-8e1e-3570286ea534)  

### 11.1.2 고급 활성화 함수  
LeakyReLU : 음수일 때 0이 되는(죽어 있는) ReLU를 보완한 것. 하이퍼파라미터 a가 leaky한 정도를 결정.  
대규모 dataset에서는 ReLU보다 항상 성능이 앞서지만, 작은 dataset에서는 과대적합의 위함이 있다.  
![image](https://github.com/user-attachments/assets/30cd0478-ed8c-46c1-aa82-236761e5855b)  

ELU : 0에서의 기울기가 불연속인 ReLU 계열의 활성화함수를 보완한 것.  
![image](https://github.com/user-attachments/assets/f961460c-c534-4163-9b5f-27a9b4f75f51)  
SELU(Scaled ELU) : 모든 은닉층이 SELU를 사용한다면 네트워크가 자기 졍규화 됨. 각 층의 평균이 0, 표준편차가 1로 된다.  
![image](https://github.com/user-attachments/assets/a6b69d6a-87cf-47b7-b39f-8f73f745b838)  

GELU : 
![image](https://github.com/user-attachments/assets/578557a0-fb85-4037-a8ed-2bcf3d1f6c4b)  
시그마 함수 : 평균이 0이고 표준편차가 1인 정규분포에서 랜덤하게 샘플링한 값이 z보다 작을 확률.  
![image](https://github.com/user-attachments/assets/b4440a0b-d2f9-4760-8d5f-2e675e37e54e)  

### 11.1.3 배치 정규화  
ReLU 계열의 활성화 함수와 He 초기화를 사용하면 초기 단계에서의 그래디언트 소실, 폭주문제를 줄일 수 있지만, 훈련하는 동안 또 발생할 수 있다.  
배치 정규화 : 각층에서 활성화 함수를 통과하기 전이나 후에 연산을 추가.  
하나는 입력을 원점에 맞추고 정규화. 하나는 각 충에서 두 개의 새로운 파라미터로 결괏값의 스케일을 조정하고 이동시킨다.  
훈련 세트를 표준화할 필요가 없다.  
![image](https://github.com/user-attachments/assets/44d02b57-1576-417a-b2ae-8e1114e37a1b)  

## 11.2 사전 훈련된 층 사용하기  
전이학습 : 해결하려는 것과 비슷한 유형의 문제를 처리한 신경망의 최상위 층을 제외하고 재사용하는 것.  
![image](https://github.com/user-attachments/assets/1af141e5-4a39-44da-9aaf-1cd3b6f89e1d)  
보통 하위 층을 많이 사용함. 새로운 작업에 유용한 고수준 특성은 원본 작업에서 유용했던 특성과는 상당히 다르기 때문.  
 - 전이학습은 작은 완전 연결 네트워크에서는 잘 작동하지 않는다.
 - 일반적인 특성을 아래쪽 층에서 감지하는 경향이 있는 심층 합성곱 신경망에서 잘 작동함.

### 11.2.2 비지도 사전 훈련  
레이블된 훈련 데이터를 충분히 모으지 못할 경우 사용.  
레이블되지 않은 훈련데이터만 많이 모을 경우, 오토인코더나 GAN판별자의 하위 층을 재사용하고 그 위에 출력층을 얹을 수도 있다.  
![image](https://github.com/user-attachments/assets/5bed3e4f-88e7-416b-b0c4-e73a38ca1bdf)  
or  
레이블된 훈련 데이터를 쉽게 얻거나 생성할 수 있는 보조 작업에서 첫 번째 신경망을 훈련하는 것.  
NLP에서는 수백만 개의 텍스트 문서로 이루어진 말뭉치를 다운로드하고 여기서 자동으로 레이블된 데이터를 생성함.  
ex) 일부 단어를 랜덤하게 지우고 예측하는 모델.  
이런 모델을 재사용 하는 것.   

## 11.3 고속 옵티마이저  
### 11.3.1 모멘텀 최적화  
 : GD와 달리 이 전의 그래디언트가 얼마였는지도 중요시하게 고려함. 모멘텀을 고려하는 것.  
 ![image](https://github.com/user-attachments/assets/6ef477e2-0aba-496a-8578-4e33767bfbdd)  
 - 경사가 가파를수록 빠르게 수렴하며 이것은 지역 최적점을 뛰어넘는 데에도 좋은 영향을 준다.  
### 11.3.2 네스테로프 가속 경사  
![image](https://github.com/user-attachments/assets/3089d889-4bbd-40da-a6b3-47398f3db1de)  

### 11.3.3 AdaGrad  
가장 가파른 경사를 좀 더 일찍 감지하고 전역 최적점 쪽으로 좀 더 빠르게 방향을 잡도록 하는 것.  
초반에는 그래디언트를 유지하다가 시간이 지나며 조금씩 줄여서 한 쪽으로 치우치지 않고 전역 최적점으로 방향을 잘 설정할 수 있도록 함.  
![image](https://github.com/user-attachments/assets/e44b617b-3fd7-477c-ad38-c6c3bd9e4ad9)  

### 11.3.4 RMSProp  
훈련 시작부터의 모든 그래디언트가 아닌 가장 최근의 반복에서 비롯된 그래디언트만 누적함.  
![image](https://github.com/user-attachments/assets/d47e86fd-3a61-4bc6-a528-3ae52c0686c3)  

### 11.3.5 Adam  
모멘텀 최적화와 RMSProp를 합친 것.  

![image](https://github.com/user-attachments/assets/5867e4fc-7e93-4cbf-9f59-612c9e7491dc)  

### 11.3.6 Adamax  
Adam에서 l2norm을 l무한대 norm으로 바꾼다. 그래서 max.  

### 11.3.9 학습률 스케줄링  
적절한 학습률을 찾는 방법에는 여러 가지가 있는데, 그중 하나가 큰 학습률로 시작하고 학습 속도가 느려질 때 학습률을 낮추는 것이다.  
이렇게 학습률을 낮추는 데에는 또 여러 전략이 있는데, 이런 전략을 학습 스케줄이라 한다.  
거듭제곱 기반 스케줄링, 지수 기반 스케줄링, 구간별 고정 스케줄링, 성능 기반 스케줄링, 1사이클 스케줄링 등이 있다.  

## 11.4 규제를 사용해 오버피팅 피하기  
### 11.4.1 l1, l2규제  
### 11.4.2 드롭아웃  
매 훈련 스텝에서 각 뉴런은 임시적으로 드롭아웃 될 확률 p를 가진다. (보통 10~50% 사이)  
![image](https://github.com/user-attachments/assets/1c3e0588-d200-4b01-aacf-1e46337c0ca3)  
--> 몇 개의 일부 뉴런에만 의존하지 않게 된다.  
그러므로 입력값의 작은 변화에 민감하지 않게 되고 안정적인 네트워크가 되어 일반화 성능이 좋아짐.  
모델이 과대적합되면 드롭아웃 비율을 늘리고 과소적합되면 드롭아웃 비율을 줄임  

### 11.4.3 몬테카를로 드롭아웃  
### 11.4.4 맥스-norm 규제  
각각 뉴런에 대해 입력의 연결 가중치 w가 ![image](https://github.com/user-attachments/assets/c6532b07-97b0-4186-be66-435efa4494fb)이 되도록 규제한다.  







